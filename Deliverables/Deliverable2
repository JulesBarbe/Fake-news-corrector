1. Problem statement:

2. Data Preprocessing: For now only using Tf-Idf Vectorizing. Realized that removing stop words might not be 
ideal as differences in the richness of the vocabulary and repetition could be a feature in itself in determining
the truth value of the article.

3. Machine learning model: Went with support vector classifiers. 

4. Preliminary results:

5. Next steps: Considering the title as a seperate feature where the pre-processing should be different, as minute differences in the title might have a big impact.
For this we could simply run a different algorithm, where we also consider the number of punctuation signs, number of letter in caps, etc.
Try different models and types of pre-processing. 
Start looking at how to do article comparison. After looking at different implementations, Latent Semantic Analysis (LSA) with use of TFIDF and Singular Value Decomposition (SVD) seems optimal.





#Jules' comment#
Either we use same or different dataset for second part of the algorithm. I found a bigger one.
Second part will probably need to be unsupervised, where we compare the ratio of buzzwords between articles with maybe similar titles/dates as the current one.
So we have to implement 1. Classification model for fake/real news and 2. Other model for article comparison. (not necessarily ML from what I read online!)
Both models will need pretty much the same pre-processing, so it would be beneficial to use the same dataset. 
For pre-processing, we try: tf-idf vectorization, n-gramming, lemmatizing/stemming, stopwords removal
For the first model, we should try VSM for this deliverable.
For second model, we try one of these I found online for text comparison: Cosine Angular Separation, Hamming Distance, Latent Semantic Analysis (LSA) 
or Latent Semantic Indexing (LSI) (none of these are machine learning though, but text-similarity measures)


